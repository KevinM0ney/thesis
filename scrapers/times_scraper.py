import requests
from datetime import datetime
import pandas as pd
import time
import os
import re
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


class NYTimesScraper:
    def __init__(self):
        self.api_key = os.getenv('NYT_API_KEY')
        print("\nâœ¨ Initializing New York Times Scraper...")
        print(f"ğŸ”‘ API Key loaded: {'Yes' if self.api_key else 'No'}")
        self.base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'
        self.start_date = '20220601'  # Format YYYYMMDD
        self.end_date = datetime.now().strftime('%Y%m%d')
        print(f"ğŸ“… Date range: {self.start_date} to {self.end_date}")
        print("=" * 100)

    def clean_html(self, text):
        # Remove HTML tags
        clean = re.compile('<.*?>')
        text = re.sub(clean, '', text)
        # Remove multiple spaces
        text = ' '.join(text.split())
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?-]', '', text)
        return text.strip()

    def fetch_articles(self, page=0):
        print(f"\nğŸ”„ Preparing to fetch page {page}...")
        params = {
            'api-key': self.api_key,
            'q': '"artificial intelligence" OR "AI"',
            'begin_date': self.start_date,
            'end_date': self.end_date,
            'page': page,
            'sort': 'newest',
            'fq': 'document_type:("article")'
        }
        print("ğŸ“ Query parameters set:")
        print(f"    - Search terms: 'artificial intelligence' OR 'AI'")
        print(f"    - Page: {page}")
        print(f"    - Sorting: Newest first")

        try:
            print("\nğŸ“¡ Sending request to NYT API...")
            response = requests.get(self.base_url, params=params)
            response.raise_for_status()
            print("âœ… Request successful!")
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ Error fetching articles: {e}")
            return None

    def process_articles(self, data):
        if not data or 'response' not in data or 'docs' not in data['response']:
            print("âŒ No data to process")
            return []

        print("\nğŸ“Š Processing articles...")
        articles = []
        total_articles = len(data['response']['docs'])
        print(f"ğŸ“š Found {total_articles} articles on this page")

        for idx, article in enumerate(data['response']['docs'], 1):
            print(f"\nğŸ“° Processing article {idx}/{total_articles}")
            print("=" * 100)

            # Get and clean the snippet
            full_text = article.get('lead_paragraph', '') or article.get('abstract', '') or article.get('snippet', '')
            full_text = self.clean_html(full_text)
            words = full_text.split()[:50]
            snippet = ' '.join(words)

            # Get the author(s)
            byline = article.get('byline', {}).get('original', '') or ''
            if byline.startswith('By '):
                byline = byline[3:]

            article_data = {
                'title': article.get('headline', {}).get('main', ''),
                'topic': article.get('section_name', ''),
                'author': byline,
                'snippet': snippet,
                'date': article.get('pub_date', ''),
                'newspaper': 'The New York Times'
            }

            # Detailed article information
            print(f"ğŸ“Œ Title: {article_data['title']}")
            print(f"ğŸ“ Topic: {article_data['topic']}")
            print(f"âœï¸ Author: {article_data['author']}")
            print(f"ğŸ“… Date: {article_data['date']}")
            print("\nğŸ“„ Snippet preview (first 50 words):")
            print(f"{snippet}...")
            print(f"\nğŸ”— Source: The New York Times")
            print("-" * 100)

            # Data validation checks
            print("\nâœ”ï¸ Data validation:")
            print(f"  - Title present: {'Yes' if article_data['title'] else 'No'}")
            print(f"  - Author present: {'Yes' if article_data['author'] else 'No'}")
            print(f"  - Topic present: {'Yes' if article_data['topic'] else 'No'}")
            print(f"  - Date present: {'Yes' if article_data['date'] else 'No'}")
            print(f"  - Content present: {'Yes' if article_data['snippet'] else 'No'}")

            articles.append(article_data)
            print(f"\nâœ… Article {idx} processed successfully")

        return articles

    def scrape_all_articles(self):
        print("\nğŸš€ Starting full scraping process...")
        all_articles = []
        page = 0
        max_pages = 100  # NYT API limit is 100 pages

        while page < max_pages:
            print(f"\nğŸ“‘ Processing page {page}...")
            print("=" * 100)

            data = self.fetch_articles(page)

            if not data:
                print("âŒ No data received, stopping scraping process")
                break

            if page == 0:
                total_hits = data['response'].get('meta', {}).get('hits', 0)
                estimated_pages = min(max_pages, (total_hits + 9) // 10)  # NYT returns 10 articles per page
                print(f"\nğŸ“Š Scraping Statistics:")
                print(f"    - Total articles found: {total_hits}")
                print(f"    - Estimated pages to fetch: {estimated_pages}")
                print(f"    - Estimated time: {estimated_pages * 12} seconds (12 second delay per page)")

            articles = self.process_articles(data)
            if not articles:
                break

            all_articles.extend(articles)

            print(f"\nğŸ“ˆ Progress: Page {page + 1}/{estimated_pages} completed")
            print(f"ğŸ“š Total articles collected so far: {len(all_articles)}")

            # NYT API rate limit is 5 calls per minute = 12 seconds between calls
            if page < max_pages - 1:
                print("\nâ³ Waiting 12 seconds before next request (respecting rate limits)...")
                time.sleep(12)

            page += 1

        print(f"\nğŸ‰ Scraping completed! Total articles collected: {len(all_articles)}")
        return all_articles

    def save_to_csv(self, articles, filename='D:/PycharmProjects/Thesis/data/nyt_ai_articles.csv'):
        print(f"\nğŸ’¾ Saving {len(articles)} articles to CSV...")
        print(f"ğŸ“‚ File path: {filename}")

        try:
            df = pd.DataFrame(articles)
            print("\nğŸ“Š DataFrame created with columns:")
            for col in df.columns:
                print(f"    - {col}: {len(df[col].unique())} unique values")

            df.to_csv(filename, index=False)
            print(f"\nâœ… Successfully saved to {filename}")
            print(f"ğŸ“Š File size: {os.path.getsize(filename) / (1024 * 1024):.2f} MB")

        except Exception as e:
            print(f"\nâŒ Error saving to CSV: {e}")


def main():
    print("\nğŸš€ Starting The New York Times AI Articles Scraper")
    print("=" * 100)

    try:
        scraper = NYTimesScraper()
        print("\nğŸ” Beginning article collection...")
        articles = scraper.scrape_all_articles()
        scraper.save_to_csv(articles)
        print("\nâœ¨ Script completed successfully!")

    except Exception as e:
        print(f"\nâŒ An error occurred during execution: {e}")

    finally:
        print("\nğŸ‘‹ Script execution ended")
        print("=" * 100)


if __name__ == "__main__":
    main()